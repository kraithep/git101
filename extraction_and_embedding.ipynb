{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: torch in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langchain-ollama) (0.3.34)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: networkx in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.1.125)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.9.2)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kraithep\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.23.4)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  16.5/16.6 MB 86.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 74.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf requests beautifulsoup4 transformers torch selenium webdriver_manager easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kraithep\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF reading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Create directories if not present\n",
    "os.makedirs(\"chunks\", exist_ok=True)\n",
    "os.makedirs(\"pdf\", exist_ok=True)  # Place your PDF files here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files.\n",
      "Total PDF chunks created: 26\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 300  # characters per chunk\n",
    "chunks_data = []  # list to store dictionaries with keys: \"chunk_name\" and \"text\"\n",
    "\n",
    "# Helper function to chunk text and return list of chunk strings.\n",
    "def chunk_text(text, chunk_size):\n",
    "    chunks = []\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        chunk = text[start:start+chunk_size]\n",
    "        if start + chunk_size < len(text):\n",
    "            # try not to cut in the middle of a word\n",
    "            last_space = chunk.rfind(\" \")\n",
    "            if last_space != -1 and last_space > 0:\n",
    "                chunk = chunk[:last_space]\n",
    "                start += last_space\n",
    "            else:\n",
    "                start += chunk_size\n",
    "        else:\n",
    "            start += chunk_size\n",
    "        chunk = chunk.strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# --- Process PDFs ---\n",
    "pdf_files = [f for f in os.listdir(\"pdf\") if f.lower().endswith(\".pdf\")]\n",
    "print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(\"pdf\", pdf_file)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pdf_basename = os.path.splitext(pdf_file)[0]\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        page_text = page.get_text()\n",
    "        if not page_text.strip():\n",
    "            continue\n",
    "        page_chunks = chunk_text(page_text, chunk_size)\n",
    "        # Save each chunk with a name like: <pdf_basename>_page<page_num>_chunk<chunk_index>.txt\n",
    "        for chunk_idx, chunk in enumerate(page_chunks, start=1):\n",
    "            chunk_filename = f\"{pdf_basename}_page{page_num}_chunk{chunk_idx}.txt\"\n",
    "            chunks_data.append({\"chunk_name\": chunk_filename, \"text\": chunk})\n",
    "    doc.close()\n",
    "print(f\"Total PDF chunks created: {len(chunks_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Chunking Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://wisesight.com/about-us/ via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /about-us/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "Captured full-page screenshot for https://wisesight.com/about-us/ as screenshot.png.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://wisesight.com/zocialeye via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /zocialeye (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/zocialeye as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/warroom via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /warroom (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/warroom as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/command-center via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /command-center (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/command-center as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/research via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /research (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/research as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/consulting via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /consulting (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/consulting as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/monitoring via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /monitoring (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/monitoring as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/chatbot-service/ via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /chatbot-service/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/chatbot-service/ as screenshot.png.\n",
      "Failed to retrieve https://wisesight.com/trend-24hours via requests: HTTPSConnectionPool(host='wisesight.com', port=443): Max retries exceeded with url: /trend-24hours (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://wisesight.com/trend-24hours as screenshot.png.\n",
      "Failed to retrieve https://thailand.zocialawards.com/2022/ via requests: HTTPSConnectionPool(host='thailand.zocialawards.com', port=443): Max retries exceeded with url: /2022/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured full-page screenshot for https://thailand.zocialawards.com/2022/ as screenshot.png.\n",
      "Total chunks after adding website content: 80\n"
     ]
    }
   ],
   "source": [
    "def create_session_with_retries():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def get_web_name(url):\n",
    "    \"\"\"Extracts a sanitized web name from a URL.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    netloc = parsed.netloc\n",
    "    if netloc.startswith(\"www.\"):\n",
    "        netloc = netloc[4:]\n",
    "    path = parsed.path.strip(\"/\")\n",
    "    if path:\n",
    "        name = f\"{netloc}_{path}\"\n",
    "    else:\n",
    "        name = netloc\n",
    "    # Remove any remaining problematic characters\n",
    "    return name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "def scrape_text(url):\n",
    "    \"\"\"Fetches and cleans text content from a URL using a session with retries.\n",
    "    If the requests-based retrieval fails, falls back to using Selenium to capture a full-page screenshot\n",
    "    and uses EasyOCR to extract text from the image.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "    \n",
    "    session = create_session_with_retries()\n",
    "    \n",
    "    try:\n",
    "        resp = session.get(url, headers=headers, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve {url} via requests: {e}\")\n",
    "        # Fallback: Use Selenium to capture a full-page screenshot and perform OCR using EasyOCR\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--window-size=1920,1080\")\n",
    "            \n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            driver.get(url)\n",
    "            # Get total scrollable height of the page\n",
    "            total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            driver.set_window_size(1920, total_height)\n",
    "            screenshot_path = \"screenshot.png\"\n",
    "            driver.save_screenshot(screenshot_path)\n",
    "            driver.quit()\n",
    "            print(f\"Captured full-page screenshot for {url} as {screenshot_path}.\")\n",
    "            \n",
    "            try:\n",
    "                import easyocr\n",
    "                # Create an EasyOCR reader object; set gpu=False if needed\n",
    "                reader = easyocr.Reader(['en','th'], gpu=False)\n",
    "                result = reader.readtext(screenshot_path, detail=0, paragraph=True)\n",
    "                text = \"\\n\".join(result)\n",
    "                os.remove(screenshot_path)\n",
    "                return text\n",
    "            except Exception as ocr_e:\n",
    "                print(f\"EasyOCR failed: {ocr_e}\")\n",
    "                os.remove(screenshot_path)\n",
    "                return \"\"\n",
    "        except Exception as selenium_e:\n",
    "            print(f\"Selenium failed for {url}: {selenium_e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.extract()\n",
    "    text = \" \".join(soup.get_text(separator=\" \").split())\n",
    "    return text\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://onlinemedia.idea2mobile.com/?p=6570\",\n",
    "    \"https://wisesight.com/about-us/\",\n",
    "    \"https://wisesight.com/zocialeye\",\n",
    "    \"https://wisesight.com/warroom\",\n",
    "    \"https://wisesight.com/command-center\",\n",
    "    \"https://wisesight.com/research\",\n",
    "    \"https://wisesight.com/consulting\",\n",
    "    \"https://wisesight.com/monitoring\",\n",
    "    \"https://wisesight.com/chatbot-service/\",\n",
    "    \"https://wisesight.com/trend-24hours\",\n",
    "    \"https://thailand.zocialawards.com/2022/\"\n",
    "]\n",
    "\n",
    "chunks_data = []  # This list will store your chunk data.\n",
    "chunk_size = 300  # Define your chunk size.\n",
    "\n",
    "def chunk_text(text, chunk_size=300):\n",
    "    chunks = []\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        chunk = text[start:start+chunk_size]\n",
    "        if start + chunk_size < len(text):\n",
    "            last_space = chunk.rfind(\" \")\n",
    "            if last_space != -1 and last_space > 0:\n",
    "                chunk = chunk[:last_space]\n",
    "                start += last_space\n",
    "            else:\n",
    "                start += chunk_size\n",
    "        else:\n",
    "            start += chunk_size\n",
    "        chunk = chunk.strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Process each URL and create chunks\n",
    "for url in urls:\n",
    "    page_text = scrape_text(url)\n",
    "    if not page_text:\n",
    "        continue\n",
    "    web_chunks = chunk_text(page_text, chunk_size)\n",
    "    web_name = get_web_name(url)  # Get a name based on the URL\n",
    "    for idx, chunk in enumerate(web_chunks, start=1):\n",
    "        # Save each file as \"{web_name}_chunk{idx}.txt\"\n",
    "        chunk_filename = f\"{web_name}_chunk{idx}.txt\"\n",
    "        chunks_data.append({\"chunk_name\": chunk_filename, \"text\": chunk})\n",
    "print(f\"Total chunks after adding website content: {len(chunks_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Chunks as Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk files\n"
     ]
    }
   ],
   "source": [
    "for entry in chunks_data:\n",
    "    file_path = os.path.join(\"chunks\", entry[\"chunk_name\"])\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry[\"text\"])\n",
    "\n",
    "print(f\"Saved chunk files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: BAAI/bge-m3\n",
      "Model loaded on device: cpu\n",
      "Computed embeddings for 80 chunks (each 1024 dimensions).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-m3\"\n",
    "print(f\"Loading embedding model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model loaded on device:\", device)\n",
    "\n",
    "# Compute embeddings in batches\n",
    "embeddings_list = []  # list of lists (each inner list is a 1024-dim embedding)\n",
    "batch_size = 16\n",
    "texts = [entry[\"text\"] for entry in chunks_data]\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the CLS token embedding (first token)\n",
    "    cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "    cls_emb = torch.nn.functional.normalize(cls_emb, p=2, dim=1)\n",
    "    cls_emb = cls_emb.cpu().numpy()\n",
    "    for emb in cls_emb:\n",
    "        embeddings_list.append(emb.tolist())\n",
    "print(f\"Computed embeddings for {len(embeddings_list)} chunks (each {len(embeddings_list[0])} dimensions).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to embeddings.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings to a CSV file with a header: first column \"chunk_name\", then columns for each dimension.\n",
    "csv_filename = \"embeddings.csv\"\n",
    "num_dims = len(embeddings_list[0])\n",
    "header = [\"chunk_name\"] + [f\"emb_{i}\" for i in range(num_dims)]\n",
    "\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for entry, emb in zip(chunks_data, embeddings_list):\n",
    "        writer.writerow([entry[\"chunk_name\"]] + emb)\n",
    "print(f\"Embeddings saved to {csv_filename}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
