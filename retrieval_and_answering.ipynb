{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-ollama torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kraithep\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: BAAI/bge-m3\n",
      "Model loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-m3\"\n",
    "print(f\"Loading embedding model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model loaded on device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_chunks(query, top_k=5):\n",
    "    # Embed the query\n",
    "    inputs = tokenizer([query], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    q_emb = output.last_hidden_state[:, 0, :]\n",
    "    q_emb = torch.nn.functional.normalize(q_emb, p=2, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Load embeddings CSV\n",
    "    df = pd.read_csv(\"embeddings.csv\")\n",
    "    # Extract embedding columns (all except the first column)\n",
    "    emb_cols = [col for col in df.columns if col.startswith(\"emb_\")]\n",
    "    chunk_embs = df[emb_cols].values  # shape: (num_chunks, num_dims)\n",
    "    \n",
    "    # Cosine similarity is just dot product (vectors are normalized)\n",
    "    sims = chunk_embs.dot(q_emb)\n",
    "    # Get indices of top K highest similarities\n",
    "    top_indices = np.argpartition(sims, -top_k)[-top_k:]\n",
    "    # Sort indices by similarity descending\n",
    "    top_indices = top_indices[np.argsort(sims[top_indices])[::-1]]\n",
    "    \n",
    "    top_results = []\n",
    "    for idx in top_indices:\n",
    "        chunk_file = df.iloc[idx][\"chunk_name\"]\n",
    "        # Read the chunk text file from the chunks/ folder\n",
    "        file_path = os.path.join(\"chunks\", chunk_file)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read().strip()\n",
    "        except Exception as e:\n",
    "            text = \"\"\n",
    "        top_results.append((text, sims[idx], chunk_file))\n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(chunks, question):\n",
    "    \"\"\"Builds a prompt using the retrieved chunks and the user's question.\"\"\"\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant for document question answering.\\n\"\n",
    "        \"Use the following context to answer the question.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Initialize the local LLM (make sure your Ollama has llama3.1:8b available)\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Showcases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wisesight คืออะไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for query: \"Wisesight คืออะไร\"\n",
      "Chunk 1 [wisesight.com_about-us_chunk1.txt, Similarity=0.620]: wisesight platform  service community  news about us careers  contact us  b...\n",
      "Chunk 2 [wisesight.com_trend-24hours_chunk1.txt, Similarity=0.548]: wisesight platform service  community  news about us  careers  contact us  ...\n",
      "Chunk 3 [thailand.zocialawards.com_2022_chunk2.txt, Similarity=0.542]: เชิดชผใช้โซเซียลมีเดียยอดเยี่ยมในสาขาต่ ไโดยได้พัฒนา wisesight meiric\" เพื่...\n",
      "Chunk 4 [wisesight.com_monitoring_chunk1.txt, Similarity=0.536]: wisesight platform  service  community news  about us  careers  contact us ...\n",
      "Chunk 5 [wisesight.com_research_chunk1.txt, Similarity=0.535]: wisesight platform  service community news about us careers  contact us  be...\n",
      "\n",
      "\n",
      "\n",
      "Final Answer from LLM:\n",
      "\n",
      "Wisesight เป็นแพลตฟอร์มบริการที่ช่วยให้สามารถจัดการโซเชียลมีเดียได้อย่างมีประสิทธิภาพ และมีเครื่องมือต่างๆ เช่น วัดผลการดัดสิน (Mentor Metric), ดัชนีชีวัด (Wisesigi) และการป้องกันความไม่พึงพอใจของลูกค้า (Crisis Management) เพื่อช่วยให้แบรนด์สามารถจัดการโซเชียลมีเดียได้อย่างมีประสิทธิภาพและปลอดภัย\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Wisesight คืออะไร\"\n",
    "top_chunks = retrieve_top_chunks(sample_query, top_k=5)\n",
    "print(f\"Top chunks for query: \\\"{sample_query}\\\"\")\n",
    "for i, (chunk, sim, fname) in enumerate(top_chunks, start=1):\n",
    "    print(f\"Chunk {i} [{fname}, Similarity={sim:.3f}]: {chunk[:75]}...\")\n",
    "\n",
    "# Retrieve text for the prompt from top chunks\n",
    "retrieved_texts = [chunk for chunk, sim, fname in top_chunks]\n",
    "prompt_text = build_prompt(retrieved_texts, sample_query)\n",
    "\n",
    "response = llm.invoke(prompt_text)\n",
    "answer = response.content if hasattr(response, \"content\") else str(response)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Final Answer from LLM:\\n\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kirin คืออะไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for query: \"Kirin คืออะไร\"\n",
      "Chunk 1 [onlinemedia.idea2mobile.com_chunk5.txt, Similarity=0.485]: Package SMEs ก็ใช้งาน KIRN A.I. ได้ เริ่มต้น 3,000 บาท ต่อเดือน (รายละเอียด...\n",
      "Chunk 2 [onlinemedia.idea2mobile.com_chunk20.txt, Similarity=0.475]: กับการใช้งานบางส่วนของ KIRIN Engine ใครยังไม่เคยลองใช้ควรลองใช้ดูครับ Post ...\n",
      "Chunk 3 [onlinemedia.idea2mobile.com_chunk4.txt, Similarity=0.464]: เข้ามาช่วยวิเคราะห์ข้อมูลจาก Social Listening ที่ทาง Wisesight ทุ่มเงินพัฒน...\n",
      "Chunk 4 [onlinemedia.idea2mobile.com_chunk8.txt, Similarity=0.436]: อยู่ตรงนี้ (ด้านขวามือ) A.I. Insight KIRIN สรุป Insigt Customer Keyword แคม...\n",
      "Chunk 5 [onlinemedia.idea2mobile.com_chunk2.txt, Similarity=0.435]: insight Content Marketing Data Report Contact us Search for: You are Here H...\n",
      "\n",
      "\n",
      "\n",
      "Final Answer from LLM:\n",
      "\n",
      "Kirin คือเครื่องมือวิเคราะห์ข้อมูลจากโซเชียลมีเดียที่พัฒนาโดย Wisesight โดยใช้เทคนิค A.I. เพื่อให้สามารถวิเคราะห์ข้อมูลได้อย่างรวดเร็วและแม่นยำ นอกจากนี้ Kirin ยังมีให้ใช้งานฟรี 7 วัน และมีราคา 3,000 บาท ต่อเดือนสำหรับ Package SMEs\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Kirin คืออะไร\"\n",
    "top_chunks = retrieve_top_chunks(sample_query, top_k=5)\n",
    "print(f\"Top chunks for query: \\\"{sample_query}\\\"\")\n",
    "for i, (chunk, sim, fname) in enumerate(top_chunks, start=1):\n",
    "    print(f\"Chunk {i} [{fname}, Similarity={sim:.3f}]: {chunk[:75]}...\")\n",
    "\n",
    "# Retrieve text for the prompt from top chunks\n",
    "retrieved_texts = [chunk for chunk, sim, fname in top_chunks]\n",
    "prompt_text = build_prompt(retrieved_texts, sample_query)\n",
    "\n",
    "response = llm.invoke(prompt_text)\n",
    "answer = response.content if hasattr(response, \"content\") else str(response)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Final Answer from LLM:\\n\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใครพัฒนาดัชนีชี้วัดเพื่อหาผู้ชนะ THAILAND ZOCIAL AWARDS 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for query: \"ใครพัฒนาดัชนีชี้วัดเพื่อหาผู้ชนะ THAILAND ZOCIAL AWARDS 2022\"\n",
      "Chunk 1 [thailand.zocialawards.com_2022_chunk1.txt, Similarity=0.598]: thailand  zocia awards ^  anniversary  live  thalland zocial awards 22 febr...\n",
      "Chunk 2 [thailand.zocialawards.com_2022_chunk9.txt, Similarity=0.481]: ถือเป็นรางวัลที่มอบให้กับแบรนด์หรือเอเจนซีเพื่อเชิดชผลงานที่ใช้โซเซียลมี เด...\n",
      "Chunk 3 [thailand.zocialawards.com_2022_chunk2.txt, Similarity=0.442]: เชิดชผใช้โซเซียลมีเดียยอดเยี่ยมในสาขาต่ ไโดยได้พัฒนา wisesight meiric\" เพื่...\n",
      "Chunk 4 [wisesight.com_command-center_chunk1.txt, Similarity=0.424]: wisesight platform servi  community about careers  contact us  become a par...\n",
      "Chunk 5 [thailand.zocialawards.com_2022_chunk3.txt, Similarity=0.419]: กาหรับการวัดผลบบ งผู่ไช้ไนปร โดยวัดผลใน 4 ช่อ แและ youtube โดยแบ่ง iric ที่...\n",
      "\n",
      "\n",
      "\n",
      "Final Answer from LLM:\n",
      "\n",
      "บริษัท วิช์ไซท์ (ประเทศไทย) จำกัด\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"ใครพัฒนาดัชนีชี้วัดเพื่อหาผู้ชนะ THAILAND ZOCIAL AWARDS 2022\"\n",
    "top_chunks = retrieve_top_chunks(sample_query, top_k=5)\n",
    "print(f\"Top chunks for query: \\\"{sample_query}\\\"\")\n",
    "for i, (chunk, sim, fname) in enumerate(top_chunks, start=1):\n",
    "    print(f\"Chunk {i} [{fname}, Similarity={sim:.3f}]: {chunk[:75]}...\")\n",
    "\n",
    "# Retrieve text for the prompt from top chunks\n",
    "retrieved_texts = [chunk for chunk, sim, fname in top_chunks]\n",
    "prompt_text = build_prompt(retrieved_texts, sample_query)\n",
    "\n",
    "response = llm.invoke(prompt_text)\n",
    "answer = response.content if hasattr(response, \"content\") else str(response)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Final Answer from LLM:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRAND METRIC คืออะไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for query: \"BRAND METRIC คืออะไร\"\n",
      "Chunk 1 [thailand.zocialawards.com_2022_chunk4.txt, Similarity=0.496]: บดคลมีผ่อเนือหาของแบรนด์และ mention สำบวนการพดถึงแบรนด์บนโซเซียลมีเดีย ประส...\n",
      "Chunk 2 [thailand.zocialawards.com_2022_chunk2.txt, Similarity=0.474]: เชิดชผใช้โซเซียลมีเดียยอดเยี่ยมในสาขาต่ ไโดยได้พัฒนา wisesight meiric\" เพื่...\n",
      "Chunk 3 [thailand.zocialawards.com_2022_chunk5.txt, Similarity=0.458]: entertainment metric ertanmen ใช่สาหรับวัด จถึงในวงกว่าง โดยมีปัจจัยในการวั...\n",
      "Chunk 4 [thailand.zocialawards.com_2022_chunk3.txt, Similarity=0.449]: กาหรับการวัดผลบบ งผู่ไช้ไนปร โดยวัดผลใน 4 ช่อ แและ youtube โดยแบ่ง iric ที่...\n",
      "Chunk 5 [thailand.zocialawards.com_2022_chunk6.txt, Similarity=0.447]: และในส่วนของกล่มรางวัลผผลิตร การออนไลน์ (online channel) จะใช่วิธีการวัดผลท...\n",
      "\n",
      "\n",
      "\n",
      "Final Answer from LLM:\n",
      "\n",
      " BRAND METRIC คือระบบการวัดผลและประเมินประสิทธิภาพของแบรนด์บนโซเชียลมีเดีย โดยใช้หลักการวัดผลที่ครอบคลุมหลายด้าน เช่น ประสิทธิภาพเชิงปริมาณ (quantity) และประสิทธิภาพเชิงคุณภาพ (quality) เพื่อให้สามารถประเมินความสำเร็จของแบรนด์ได้อย่างรอบคอบ\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"BRAND METRIC คืออะไร\"\n",
    "top_chunks = retrieve_top_chunks(sample_query, top_k=5)\n",
    "print(f\"Top chunks for query: \\\"{sample_query}\\\"\")\n",
    "for i, (chunk, sim, fname) in enumerate(top_chunks, start=1):\n",
    "    print(f\"Chunk {i} [{fname}, Similarity={sim:.3f}]: {chunk[:75]}...\")\n",
    "\n",
    "# Retrieve text for the prompt from top chunks\n",
    "retrieved_texts = [chunk for chunk, sim, fname in top_chunks]\n",
    "prompt_text = build_prompt(retrieved_texts, sample_query)\n",
    "\n",
    "response = llm.invoke(prompt_text)\n",
    "answer = response.content if hasattr(response, \"content\") else str(response)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Final Answer from LLM:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTERTAINMENT METRIC คืออะไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for query: \"ENTERTAINMENT METRIC คืออะไร\"\n",
      "Chunk 1 [thailand.zocialawards.com_2022_chunk5.txt, Similarity=0.600]: entertainment metric ertanmen ใช่สาหรับวัด จถึงในวงกว่าง โดยมีปัจจัยในการวั...\n",
      "Chunk 2 [thailand.zocialawards.com_2022_chunk6.txt, Similarity=0.476]: และในส่วนของกล่มรางวัลผผลิตร การออนไลน์ (online channel) จะใช่วิธีการวัดผลท...\n",
      "Chunk 3 [thailand.zocialawards.com_2022_chunk3.txt, Similarity=0.459]: กาหรับการวัดผลบบ งผู่ไช้ไนปร โดยวัดผลใน 4 ช่อ แและ youtube โดยแบ่ง iric ที่...\n",
      "Chunk 4 [onlinemedia.idea2mobile.com_chunk6.txt, Similarity=0.450]: วัดจาก Message ที่เข้ามาในระบบเปรียบเทียบสัดส่วน 4.Engagement การมีส่วนร่วม...\n",
      "Chunk 5 [thailand.zocialawards.com_2022_chunk2.txt, Similarity=0.449]: เชิดชผใช้โซเซียลมีเดียยอดเยี่ยมในสาขาต่ ไโดยได้พัฒนา wisesight meiric\" เพื่...\n",
      "\n",
      "\n",
      "\n",
      "Final Answer from LLM:\n",
      "\n",
      "ENTERTAINMENT METRIC เป็นระบบการวัดผลในการประเมินความสำเร็จของผลงานในวงการบันเทิง โดยมีปัจจัยในการวัดผลที่เกี่ยวข้องกับกลุ่มเป้าหมาย, ระดับการกระจายเนื้อหาผ่านโซเชียลมีเดีย, ปริมาณการพูดถึงผลงานของดารา นักแสดง ศิลปิน ละคร ดังๆ และปริมาณการแชร์บนโซเชียลมีเดีย\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"ENTERTAINMENT METRIC คืออะไร\"\n",
    "top_chunks = retrieve_top_chunks(sample_query, top_k=5)\n",
    "print(f\"Top chunks for query: \\\"{sample_query}\\\"\")\n",
    "for i, (chunk, sim, fname) in enumerate(top_chunks, start=1):\n",
    "    print(f\"Chunk {i} [{fname}, Similarity={sim:.3f}]: {chunk[:75]}...\")\n",
    "\n",
    "# Retrieve text for the prompt from top chunks\n",
    "retrieved_texts = [chunk for chunk, sim, fname in top_chunks]\n",
    "prompt_text = build_prompt(retrieved_texts, sample_query)\n",
    "\n",
    "response = llm.invoke(prompt_text)\n",
    "answer = response.content if hasattr(response, \"content\") else str(response)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Final Answer from LLM:\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะ maximize customer satisfaction ได้ยังไง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for query: \"เราจะ maximize customer satisfaction ได้ยังไง\"\n",
      "Chunk 1 [wisesight.com_about-us_chunk2.txt, Similarity=0.515]: have been growing strongly. over 14 years  experience, we have collected  t...\n",
      "Chunk 2 [wisesight.com_warroom_chunk2.txt, Similarity=0.510]: social channel  warroom helns you maximize customer satisfaction as the pla...\n",
      "Chunk 3 [wisesight.com_consulting_chunk7.txt, Similarity=0.500]: gain the riaht un to date and useful consumer insiahts instantly in time fo...\n",
      "Chunk 4 [wisesight.com_warroom_chunk1.txt, Similarity=0.499]: wisesight plateopm  sepvicf  coumumtv  mfws  about rapfeps  routart us เ06เ...\n",
      "Chunk 5 [wisesight.com_about-us_chunk5.txt, Similarity=0.499]: more about our cookie policy  allow us...\n",
      "\n",
      "\n",
      "\n",
      "Final Answer from LLM:\n",
      "\n",
      "เราสามารถ maximize customer satisfaction ได้โดยใช้ Social Channel Warroom ซึ่งเป็นแพลตฟอร์มที่ช่วยให้คุณสามารถจัดการและตรวจสอบการกล่าวถึงของลูกค้าในโซเชียลมีเดียได้อย่างครอบคลุม และยังช่วยให้คุณสามารถตอบสนองความต้องการของลูกค้าอย่างรวดเร็วและแม่นยำ\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"เราจะ maximize customer satisfaction ได้ยังไง\"\n",
    "top_chunks = retrieve_top_chunks(sample_query, top_k=5)\n",
    "print(f\"Top chunks for query: \\\"{sample_query}\\\"\")\n",
    "for i, (chunk, sim, fname) in enumerate(top_chunks, start=1):\n",
    "    print(f\"Chunk {i} [{fname}, Similarity={sim:.3f}]: {chunk[:75]}...\")\n",
    "\n",
    "# Retrieve text for the prompt from top chunks\n",
    "retrieved_texts = [chunk for chunk, sim, fname in top_chunks]\n",
    "prompt_text = build_prompt(retrieved_texts, sample_query)\n",
    "\n",
    "response = llm.invoke(prompt_text)\n",
    "answer = response.content if hasattr(response, \"content\") else str(response)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Final Answer from LLM:\\n\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
